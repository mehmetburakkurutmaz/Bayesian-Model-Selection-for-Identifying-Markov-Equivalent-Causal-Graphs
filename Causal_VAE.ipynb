{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Causal_VAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX5JehBgWl8P",
        "colab_type": "code",
        "outputId": "c6e6cd05-7890-4603-98c5-34cbd7ffe031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "source": [
        "!pip install --upgrade tensorflow-gpu tensorflow-probability tensorflow-datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already up-to-date: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already up-to-date: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle==1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (19.1.0)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2019.9.11)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YOnLWk3WWm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability import distributions as tfd\n",
        "from tensorflow_probability import layers as tfpl\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow import math as tfm\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow import nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI5vZN7H_poa",
        "colab_type": "text"
      },
      "source": [
        "# Dense Variational Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crqwSVls_ofo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseVariational(tfkl.Layer):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        activation=tf.identity,\n",
        "        activity_regularizer=None,\n",
        "        kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "        kernel_posterior_tensor_fn=lambda d: d.sample(),\n",
        "        kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "        kernel_divergence_fn=tfd.kl_divergence,\n",
        "        bias_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "        bias_posterior_tensor_fn=lambda d: d.sample(),\n",
        "        bias_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "        bias_divergence_fn=tfd.kl_divergence,\n",
        "        kl_weight = 1.,\n",
        "        **kwargs):\n",
        "        super(DenseVariational, self).__init__(activity_regularizer=activity_regularizer,**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tfk.activations.get(activation)\n",
        "        self.input_spec = tfkl.InputSpec(min_ndim=2)\n",
        "        self.kernel_posterior_fn = kernel_posterior_fn\n",
        "        self.kernel_posterior_tensor_fn = kernel_posterior_tensor_fn\n",
        "        self.kernel_prior_fn = kernel_prior_fn\n",
        "        self.bias_posterior_fn = bias_posterior_fn\n",
        "        self.bias_posterior_tensor_fn = bias_posterior_tensor_fn\n",
        "        self.bias_prior_fn = bias_prior_fn\n",
        "        self.kernel_divergence_fn = kernel_divergence_fn\n",
        "        self.bias_divergence_fn = bias_divergence_fn\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        in_size = input_shape[-1]\n",
        "        \n",
        "        self._input_spec = tfkl.InputSpec(min_ndim=2, axes={-1: in_size})\n",
        "\n",
        "        dtype = tf.as_dtype(self.dtype or tfk.backend.floatx())\n",
        "\n",
        "        self.kernel_posterior = self.kernel_posterior_fn(\n",
        "                dtype, [in_size, self.units], 'kernel_posterior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.kernel_prior = self.kernel_prior_fn(\n",
        "                dtype, [in_size, self.units], 'kernel_prior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.bias_posterior = self.bias_posterior_fn(\n",
        "                dtype, [self.units], 'bias_posterior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.bias_prior = self.bias_prior_fn(\n",
        "                dtype, [self.units], 'bias_prior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.convert_to_tensor(value=inputs, dtype=self.dtype)\n",
        "\n",
        "        outputs = self._apply_variational_kernel(inputs)\n",
        "        outputs = self._apply_variational_bias(outputs)\n",
        "        outputs = self.activation(outputs)\n",
        "        self._apply_divergence(\n",
        "            self.kernel_divergence_fn,\n",
        "            self.kernel_posterior,\n",
        "            self.kernel_prior,\n",
        "            name='divergence_kernel')\n",
        "        self._apply_divergence(\n",
        "            self.bias_divergence_fn,\n",
        "            self.bias_posterior,\n",
        "            self.bias_prior,\n",
        "            name='divergence_bias')\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        input_shape = input_shape.with_rank_at_least(2)\n",
        "        return input_shape[:-1].concatenate(self.units)\n",
        "\n",
        "    def _apply_variational_kernel(self, inputs):\n",
        "        self.kernel_posterior_tensor = self.kernel_posterior_tensor_fn(self.kernel_posterior)\n",
        "        self.kernel_posterior_affine = None\n",
        "        self.kernel_posterior_affine_tensor = None\n",
        "        return self._matmul(inputs, self.kernel_posterior_tensor)\n",
        "\n",
        "    def _apply_variational_bias(self, inputs):\n",
        "        self.bias_posterior_tensor = self.bias_posterior_tensor_fn(self.bias_posterior)\n",
        "        return tf.nn.bias_add(inputs, self.bias_posterior_tensor)\n",
        "\n",
        "    def _apply_divergence(self, divergence_fn, posterior, prior, name):\n",
        "        divergence = self.kl_weight*tf.identity(divergence_fn(posterior, prior), name=name)\n",
        "        self.add_loss(divergence)\n",
        "   \n",
        "    def _matmul(self, inputs, kernel):\n",
        "        if inputs.shape.ndims <= 2:\n",
        "            return tf.matmul(inputs, kernel)\n",
        "        # To handle broadcasting, we must use `tensordot`.\n",
        "        return tf.tensordot(inputs, kernel, axes=[[-1], [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lfB9ZseAKU8",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Variational Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulkDaxvcAIFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvVariational(tfkl.Layer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            rank,\n",
        "            filters,\n",
        "            kernel_size,\n",
        "            strides=1,\n",
        "            padding='valid',\n",
        "            data_format='channels_last',\n",
        "            dilations=1,\n",
        "            activation=tf.identity,\n",
        "            activity_regularizer=None,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_posterior_tensor_fn=lambda d: d.sample(),\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_divergence_fn=tfd.kl_divergence,\n",
        "            kl_weight = 1.,\n",
        "            **kwargs):\n",
        "\n",
        "        super(ConvVariational, self).__init__(activity_regularizer=activity_regularizer,**kwargs)\n",
        "        self.rank = rank\n",
        "        self.filters = filters\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.dilations = conv_utils.normalize_tuple(dilations, rank, 'dilations')\n",
        "        self.activation = tfk.activations.get(activation)\n",
        "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "        self.kernel_posterior_fn = kernel_posterior_fn\n",
        "        self.kernel_posterior_tensor_fn = kernel_posterior_tensor_fn\n",
        "        self.kernel_prior_fn = kernel_prior_fn\n",
        "        self.kernel_divergence_fn = kernel_divergence_fn\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        channel_axis = 1 if self.data_format == 'channels_first' else -1\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        \n",
        "        self.input_spec = tfkl.InputSpec(ndim=self.rank + 2, axes={channel_axis: input_dim})\n",
        "    \n",
        "        # If self.dtype is None, build weights using the default dtype.\n",
        "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
        "\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        # Must have a posterior kernel.\n",
        "        self.kernel_posterior = self.kernel_posterior_fn(\n",
        "                dtype, kernel_shape, 'kernel_posterior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.kernel_prior = self.kernel_prior_fn(\n",
        "                dtype, kernel_shape, 'kernel_prior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.convert_to_tensor(value=inputs, dtype=self.dtype)\n",
        "        outputs = self._apply_variational_kernel(inputs)\n",
        "        outputs = self.activation(outputs)\n",
        "        self._apply_divergence(self.kernel_divergence_fn,\n",
        "                                   self.kernel_posterior,\n",
        "                                   self.kernel_prior,\n",
        "                                   name='divergence_kernel')\n",
        "        return outputs\n",
        "\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tf.TensorShape(input_shape).as_list()\n",
        "        shape = input_shape[1:-1] if self.data_format == 'channels_last' else input_shape[2:] \n",
        "        \n",
        "        output_shape = [conv_utils.conv_output_length(\n",
        "                                dim,\n",
        "                                self.kernel_size[i],\n",
        "                                padding=self.padding,\n",
        "                                stride=self.strides[i],\n",
        "                                dilation=self.dilation_rate[i])\n",
        "                        for i,dim in enumerate(shape)]\n",
        "        \n",
        "        output_shape = tf.TensorShape([input_shape[0]] + output_shape + [self.filters]) \\\n",
        "                    if self.data_format == 'channels_last' else \\\n",
        "                    tf.TensorShape([input_shape[0], self.filters] + output_shape)\n",
        "\n",
        "        return output_shape\n",
        "     \n",
        "    def _apply_variational_kernel(self, inputs):\n",
        "        self.kernel_posterior_tensor = self.kernel_posterior_tensor_fn(self.kernel_posterior)\n",
        "        self.kernel_posterior_affine = None\n",
        "        self.kernel_posterior_affine_tensor = None\n",
        "        outputs = nn.convolution(inputs, self.kernel_posterior_tensor,\n",
        "                                 dilations=self.dilations,\n",
        "                                 strides=self.strides,\n",
        "                                 padding=self.padding.upper(),\n",
        "                                 data_format=conv_utils.convert_data_format(\n",
        "                                 self.data_format, self.rank + 2))\n",
        "        return outputs\n",
        "\n",
        "    def _apply_divergence(self, divergence_fn, posterior, prior, name):\n",
        "        divergence = self.kl_weight*tf.identity(divergence_fn(posterior, prior), name=name)\n",
        "        self.add_loss(divergence)\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD_CLtx5AW4L",
        "colab_type": "text"
      },
      "source": [
        "# Deconvolutional Variational Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeFP1F2zAkoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeconvVariational(tfkl.Layer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            rank,\n",
        "            filters,\n",
        "            kernel_size,\n",
        "            strides=1,\n",
        "            padding='valid',\n",
        "            output_padding=None,\n",
        "            data_format='channels_last',\n",
        "            dilation=1,\n",
        "            activation=tf.identity,\n",
        "            activity_regularizer=None,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_posterior_tensor_fn=lambda d: d.sample(),\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_divergence_fn=tfd.kl_divergence,\n",
        "            kl_weight = 1.,\n",
        "            **kwargs):\n",
        "        \n",
        "        super(DeconvVariational, self).__init__(activity_regularizer=activity_regularizer,**kwargs)\n",
        "        self.rank = rank\n",
        "        self.filters = filters\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "        self.dilation = conv_utils.normalize_tuple(dilation, rank, 'dilations')\n",
        "        self.activation = tfk.activations.get(activation)\n",
        "        self.kernel_posterior_fn = kernel_posterior_fn\n",
        "        self.kernel_posterior_tensor_fn = kernel_posterior_tensor_fn\n",
        "        self.kernel_prior_fn = kernel_prior_fn\n",
        "        self.kernel_divergence_fn = kernel_divergence_fn\n",
        "        self.kl_weight = kl_weight\n",
        "        self.output_padding = output_padding\n",
        "        if self.output_padding is not None:\n",
        "            self.output_padding = conv_utils.normalize_tuple(self.output_padding, rank, 'output_padding')\n",
        "        else:\n",
        "            self.output_padding = [None]*rank\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        input_dim = input_shape[-1]\n",
        "        \n",
        "        self.input_spec = tfkl.InputSpec(ndim=self.rank+2, axes={-1: input_dim})\n",
        "        \n",
        "        # If self.dtype is None, build weights using the default dtype.\n",
        "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
        "        \n",
        "        kernel_shape = self.kernel_size + (self.filters, input_dim)\n",
        "        \n",
        "        # Must have a posterior kernel.\n",
        "        self.kernel_posterior = self.kernel_posterior_fn(\n",
        "                dtype, kernel_shape, 'kernel_posterior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.kernel_prior = self.kernel_prior_fn(\n",
        "                dtype, kernel_shape, 'kernel_prior',\n",
        "                self.trainable, self.add_weight)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.kernel_posterior_tensor = self.kernel_posterior_tensor_fn(self.kernel_posterior)\n",
        "        \n",
        "        inputs_shape = array_ops.shape(inputs)\n",
        "        shape = inputs_shape[1:-1] if self.data_format == 'channels_last' else inputs_shape[2:]\n",
        "                \n",
        "        spatial_shape = [conv_utils.deconv_output_length(\n",
        "                                shape[r],\n",
        "                                self.kernel_size[r],\n",
        "                                padding=self.padding,\n",
        "                                output_padding=self.output_padding[r],\n",
        "                                stride=self.strides[r],\n",
        "                                dilation=self.dilation[r])\n",
        "                        for r in range(self.rank)]\n",
        "        \n",
        "        output_shape = [inputs_shape[0]] + spatial_shape + [self.filters] \\\n",
        "                    if self.data_format == 'channels_last' else \\\n",
        "                    [inputs_shape[0], self.filters] + spatial_shape\n",
        "        \n",
        "        outputs = nn.conv_transpose(inputs, self.kernel_posterior_tensor,\n",
        "                                    output_shape=output_shape,\n",
        "                                    padding=self.padding.upper(),\n",
        "                                    dilations=self.dilation,\n",
        "                                    strides=self.strides,\n",
        "                                    data_format=conv_utils.convert_data_format(\n",
        "                                    self.data_format, self.rank + 2))\n",
        "        \n",
        "        outputs = self.activation(outputs)\n",
        "        \n",
        "        self._apply_divergence(self.kernel_divergence_fn,\n",
        "                                   self.kernel_posterior,\n",
        "                                   self.kernel_prior,\n",
        "                                   name='divergence_kernel')\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        shape = input_shape[1:-1] if self.data_format == 'channels_last' else input_shape[2:]\n",
        "        \n",
        "        output_shape = [conv_utils.deconv_output_length(\n",
        "                                dim,\n",
        "                                self.kernel_size[i],\n",
        "                                padding=self.padding,\n",
        "                                output_padding=self.output_padding[i],\n",
        "                                stride=self.strides[i],\n",
        "                                dilation=self.dilation[i])\n",
        "                        for i,dim in enumerate(shape)]\n",
        "        \n",
        "        output_shape = tf.TensorShape([input_shape[0]] + output_shape + [self.filters]) \\\n",
        "                    if self.data_format == 'channels_last' else \\\n",
        "                    tf.TensorShape([input_shape[0], self.filters] + output_shape)\n",
        "\n",
        "        return output_shape\n",
        "\n",
        "    def _apply_divergence(self, divergence_fn, posterior, prior, name):\n",
        "        divergence = self.kl_weight*tf.identity(divergence_fn(posterior, prior), name=name)\n",
        "        self.add_loss(divergence)\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQBQHfYCA7Zr",
        "colab_type": "text"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY9UTb9kCmLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(batch):\n",
        "  imgs = batch.reshape(-1,28,28,1).astype(\"float32\")\n",
        "  imgs[imgs < 128] = 0.\n",
        "  imgs[imgs >= 128] = 1.\n",
        "  return tf.cast(imgs,tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shIWsSQvEYxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatches(batch,mb_size=1):\n",
        "  return tf.data.Dataset.from_tensor_slices((batch,batch)).batch(mb_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBMUgzFZWzTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = 10\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#train_images = train_images[:N,...]\n",
        "#train_labels = train_labels[:N,...]\n",
        "\n",
        "train_images = [preprocess(train_images[train_labels == d]) for d in range(D)]\n",
        "test_images = [preprocess(test_images[test_labels == d]) for d in range(D)]\n",
        "\n",
        "train_sizes = [np.sum(train_labels == d) for d in range(D)]\n",
        "test_sizes = [np.sum(test_labels == d) for d in range(D)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-WIpEv7Zbcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "train_data = [minibatches(batch,BATCH_SIZE) for batch in train_images]\n",
        "test_data = [minibatches(batch,BATCH_SIZE) for batch in test_images]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0imerJhZmg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = train_images[0].shape[1:]\n",
        "encoded_size = 8\n",
        "base_depth = 24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6lTd3DEJ_in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_encoder(input_shape, latent_dim, base_filters, kl_weight=1.0):\n",
        "    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),reinterpreted_batch_ndims=1)\n",
        "    encoder = tfk.Sequential([\n",
        "      tfkl.InputLayer(input_shape=input_shape),\n",
        "      tfkl.Lambda(lambda x: x - 0.5),\n",
        "      tfkl.Conv2D(base_filters, 5, strides=2,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "      #tfkl.Conv2D(base_filters, 5, strides=2,\n",
        "      #          padding='same', activation=tf.nn.leaky_relu),\n",
        "      #tfkl.Conv2D(2 * base_filters, 5, strides=1,\n",
        "      #          padding='same', activation=tf.nn.leaky_relu),\n",
        "      tfkl.Conv2D(2 * base_filters, 5, strides=2,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "      tfkl.Conv2D(4 * latent_dim, 7, strides=2,\n",
        "                padding='valid', activation=tf.nn.leaky_relu),\n",
        "      tfkl.Flatten(),\n",
        "      tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None),\n",
        "      tfpl.MultivariateNormalTriL(latent_dim,\n",
        "          activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=kl_weight)),])\n",
        "    return encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u0MEbz8Kr8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_decoder(output_shape, latent_dim, base_filters, kl_weight=1.0):\n",
        "    decoder = tfk.Sequential([\n",
        "      tfkl.InputLayer(input_shape=[latent_dim]),\n",
        "      tfkl.Reshape([1, 1, latent_dim]),\n",
        "      DeconvVariational(2, 2 * base_filters, 7, strides=2, kl_weight=kl_weight,\n",
        "                          padding='valid', activation=tf.nn.leaky_relu),\n",
        "      #DeconvVariational(2, 2 * base_filters, 5, strides=1, kl_weight=kl_weight,\n",
        "      #                    padding='same', activation=tf.nn.leaky_relu),\n",
        "      DeconvVariational(2, 2 * base_filters, 5, strides=2, kl_weight=kl_weight,\n",
        "                          padding='same', activation=tf.nn.leaky_relu),\n",
        "      #DeconvVariational(2, base_filters, 5, strides=1, kl_weight=kl_weight,\n",
        "      #                    padding='same', activation=tf.nn.leaky_relu),\n",
        "      DeconvVariational(2, base_filters, 5, strides=2, kl_weight=kl_weight,\n",
        "                          padding='same', activation=tf.nn.leaky_relu),\n",
        "      #DeconvVariational(2, base_filters, 5, strides=1, kl_weight=kl_weight,\n",
        "      #                    padding='same', activation=tf.nn.leaky_relu),\n",
        "      ConvVariational(2, filters=1, kernel_size=5, strides=1, kl_weight=kl_weight,\n",
        "                          padding='same', activation=None),\n",
        "      tfkl.Flatten(),\n",
        "      tfpl.IndependentBernoulli(output_shape, tfd.Bernoulli.logits),])\n",
        "    return decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0zflIMdZtt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoders = [create_encoder(img_shape,encoded_size,base_depth,kl_weight=1) for d in range(D)]\n",
        "decoders = [create_decoder(img_shape,encoded_size,base_depth,kl_weight=1/train_sizes[d]) for d in range(D)]\n",
        "\n",
        "models = [tfk.Model(inputs=encoders[d].inputs, outputs=decoders[d](encoders[d].outputs)) for d in range(D)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBks33X6ZzPS",
        "colab_type": "code",
        "outputId": "84adc7db-9280-4970-9eec-29f969943082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "\n",
        "for d in range(D):\n",
        "  print(\"Training for digit: \",d)\n",
        "  print(\"-\"*40)\n",
        "  models[d].compile(optimizer=tf.optimizers.Adam(learning_rate=1e-2), loss=negative_log_likelihood)\n",
        "  models[d].fit(train_data[d], epochs=15, validation_data=test_data[d])\n",
        "  print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for digit:  0\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 7s 115ms/step - loss: 368.3497 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 233.7081 - val_loss: 220.9423\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 208.9553 - val_loss: 196.6602\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 195.4986 - val_loss: 191.9420\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 190.7680 - val_loss: 187.7291\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 187.6140 - val_loss: 183.2626\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 182.2551 - val_loss: 179.2867\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 179.0519 - val_loss: 177.6850\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 178.4163 - val_loss: 181.2046\n",
            "Epoch 10/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 177.0245 - val_loss: 178.2350\n",
            "Epoch 11/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 176.5228 - val_loss: 175.1167\n",
            "Epoch 12/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 174.3539 - val_loss: 172.0499\n",
            "Epoch 13/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 172.8760 - val_loss: 173.4202\n",
            "Epoch 14/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 171.1755 - val_loss: 171.9134\n",
            "Epoch 15/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 171.1102 - val_loss: 172.9580\n",
            "\n",
            "Training for digit:  1\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "68/68 [==============================] - 6s 85ms/step - loss: 202.8457 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 112.5844 - val_loss: 105.3470\n",
            "Epoch 3/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 104.7417 - val_loss: 99.6047\n",
            "Epoch 4/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 100.3164 - val_loss: 98.2941\n",
            "Epoch 5/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 98.6554 - val_loss: 97.8010\n",
            "Epoch 6/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 97.0194 - val_loss: 94.8211\n",
            "Epoch 7/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 96.4618 - val_loss: 95.6019\n",
            "Epoch 8/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 95.0853 - val_loss: 93.2931\n",
            "Epoch 9/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 94.1482 - val_loss: 92.3116\n",
            "Epoch 10/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 93.5487 - val_loss: 90.5020\n",
            "Epoch 11/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 92.4435 - val_loss: 91.4517\n",
            "Epoch 12/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 91.0275 - val_loss: 88.2680\n",
            "Epoch 13/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 89.6618 - val_loss: 86.1491\n",
            "Epoch 14/15\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 88.4927 - val_loss: 86.0674\n",
            "Epoch 15/15\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 86.8304 - val_loss: 86.6162\n",
            "\n",
            "Training for digit:  2\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 6s 93ms/step - loss: 324.5020 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 251.8623 - val_loss: 234.4468\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 228.1047 - val_loss: 219.9374\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 220.5675 - val_loss: 216.9509\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 212.4897 - val_loss: 209.2952\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 207.5052 - val_loss: 202.5886\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 204.5253 - val_loss: 198.1203\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 198.8449 - val_loss: 195.5908\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 194.3767 - val_loss: 192.3910\n",
            "Epoch 10/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 194.0390 - val_loss: 193.6238\n",
            "Epoch 11/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 194.1750 - val_loss: 191.3554\n",
            "Epoch 12/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 189.2993 - val_loss: 186.0702\n",
            "Epoch 13/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 189.9187 - val_loss: 193.7419\n",
            "Epoch 14/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 187.9277 - val_loss: 183.6529\n",
            "Epoch 15/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 185.7906 - val_loss: 187.3773\n",
            "\n",
            "Training for digit:  3\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "62/62 [==============================] - 6s 91ms/step - loss: 333.9625 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 231.2148 - val_loss: 217.9635\n",
            "Epoch 3/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 213.2881 - val_loss: 209.0054\n",
            "Epoch 4/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 205.2063 - val_loss: 198.1710\n",
            "Epoch 5/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 196.3957 - val_loss: 194.1894\n",
            "Epoch 6/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 194.4968 - val_loss: 192.6116\n",
            "Epoch 7/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 191.2872 - val_loss: 191.7436\n",
            "Epoch 8/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 190.8212 - val_loss: 188.8667\n",
            "Epoch 9/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 187.6026 - val_loss: 189.0108\n",
            "Epoch 10/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 185.5374 - val_loss: 182.1984\n",
            "Epoch 11/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 185.3761 - val_loss: 182.7102\n",
            "Epoch 12/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 183.3566 - val_loss: 184.0788\n",
            "Epoch 13/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 181.7485 - val_loss: 180.4962\n",
            "Epoch 14/15\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 181.2230 - val_loss: 181.6247\n",
            "Epoch 15/15\n",
            "62/62 [==============================] - 2s 27ms/step - loss: 180.0711 - val_loss: 179.2754\n",
            "\n",
            "Training for digit:  4\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "59/59 [==============================] - 5s 86ms/step - loss: 313.4756 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 224.7740 - val_loss: 207.0958\n",
            "Epoch 3/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 200.8301 - val_loss: 191.4951\n",
            "Epoch 4/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 191.0827 - val_loss: 187.3974\n",
            "Epoch 5/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 185.7749 - val_loss: 181.4103\n",
            "Epoch 6/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 181.7697 - val_loss: 182.5694\n",
            "Epoch 7/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 179.6809 - val_loss: 175.2835\n",
            "Epoch 8/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 175.5419 - val_loss: 171.8809\n",
            "Epoch 9/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 172.3782 - val_loss: 171.9894\n",
            "Epoch 10/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 170.0610 - val_loss: 166.1385\n",
            "Epoch 11/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 166.6727 - val_loss: 168.3622\n",
            "Epoch 12/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 165.0488 - val_loss: 162.9609\n",
            "Epoch 13/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 163.2506 - val_loss: 162.2198\n",
            "Epoch 14/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 162.1104 - val_loss: 158.9768\n",
            "Epoch 15/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 160.3520 - val_loss: 159.4285\n",
            "\n",
            "Training for digit:  5\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "55/55 [==============================] - 6s 102ms/step - loss: 349.1315 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "55/55 [==============================] - 2s 28ms/step - loss: 236.5381 - val_loss: 224.0047\n",
            "Epoch 3/15\n",
            "55/55 [==============================] - 1s 27ms/step - loss: 218.6109 - val_loss: 214.5210\n",
            "Epoch 4/15\n",
            "55/55 [==============================] - 1s 27ms/step - loss: 212.1467 - val_loss: 214.2940\n",
            "Epoch 5/15\n",
            "55/55 [==============================] - 2s 27ms/step - loss: 204.0883 - val_loss: 203.2187\n",
            "Epoch 6/15\n",
            "55/55 [==============================] - 1s 27ms/step - loss: 198.5237 - val_loss: 200.8378\n",
            "Epoch 7/15\n",
            "55/55 [==============================] - 1s 27ms/step - loss: 196.6741 - val_loss: 196.1866\n",
            "Epoch 8/15\n",
            "55/55 [==============================] - 1s 27ms/step - loss: 194.3924 - val_loss: 193.9260\n",
            "Epoch 9/15\n",
            "55/55 [==============================] - 1s 27ms/step - loss: 191.1575 - val_loss: 194.0709\n",
            "Epoch 10/15\n",
            "55/55 [==============================] - 2s 28ms/step - loss: 189.6601 - val_loss: 189.3155\n",
            "Epoch 11/15\n",
            "55/55 [==============================] - 2s 28ms/step - loss: 186.9320 - val_loss: 183.9375\n",
            "Epoch 12/15\n",
            "55/55 [==============================] - 2s 28ms/step - loss: 185.8472 - val_loss: 184.0023\n",
            "Epoch 13/15\n",
            "55/55 [==============================] - 2s 29ms/step - loss: 183.3887 - val_loss: 184.3775\n",
            "Epoch 14/15\n",
            "55/55 [==============================] - 2s 28ms/step - loss: 181.8662 - val_loss: 187.0392\n",
            "Epoch 15/15\n",
            "55/55 [==============================] - 2s 29ms/step - loss: 181.2107 - val_loss: 182.7242\n",
            "\n",
            "Training for digit:  6\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 5s 86ms/step - loss: 370.6775 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 218.3684 - val_loss: 209.1528\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 195.8952 - val_loss: 193.1638\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 188.9232 - val_loss: 188.1269\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 183.9686 - val_loss: 185.0048\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 180.3118 - val_loss: 187.8196\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 180.3561 - val_loss: 180.4791\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 174.7999 - val_loss: 176.9630\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 174.1653 - val_loss: 175.5294\n",
            "Epoch 10/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 172.3067 - val_loss: 172.0282\n",
            "Epoch 11/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 168.5731 - val_loss: 172.8879\n",
            "Epoch 12/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 165.9916 - val_loss: 167.4893\n",
            "Epoch 13/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 164.3382 - val_loss: 166.3418\n",
            "Epoch 14/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 161.7282 - val_loss: 166.5666\n",
            "Epoch 15/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 159.4765 - val_loss: 158.8083\n",
            "\n",
            "Training for digit:  7\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "63/63 [==============================] - 6s 94ms/step - loss: 303.2629 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 195.7958 - val_loss: 175.5004\n",
            "Epoch 3/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 169.8778 - val_loss: 164.5394\n",
            "Epoch 4/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 164.7006 - val_loss: 159.4494\n",
            "Epoch 5/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 163.0526 - val_loss: 157.8635\n",
            "Epoch 6/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 159.0358 - val_loss: 154.3481\n",
            "Epoch 7/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 155.4775 - val_loss: 155.0547\n",
            "Epoch 8/15\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 152.1530 - val_loss: 145.4960\n",
            "Epoch 9/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 148.3149 - val_loss: 147.3216\n",
            "Epoch 10/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 146.3466 - val_loss: 141.8654\n",
            "Epoch 11/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 146.7912 - val_loss: 147.4868\n",
            "Epoch 12/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 145.1278 - val_loss: 141.6075\n",
            "Epoch 13/15\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 143.3813 - val_loss: 140.6973\n",
            "Epoch 14/15\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 143.3587 - val_loss: 142.2634\n",
            "Epoch 15/15\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 141.7417 - val_loss: 139.3730\n",
            "\n",
            "Training for digit:  8\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "59/59 [==============================] - 5s 90ms/step - loss: 334.8883 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 232.4990 - val_loss: 222.9367\n",
            "Epoch 3/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 215.6515 - val_loss: 210.8051\n",
            "Epoch 4/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 209.5754 - val_loss: 208.8489\n",
            "Epoch 5/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 204.3068 - val_loss: 203.5353\n",
            "Epoch 6/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 199.5396 - val_loss: 199.5804\n",
            "Epoch 7/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 197.2389 - val_loss: 199.9141\n",
            "Epoch 8/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 197.3603 - val_loss: 196.9004\n",
            "Epoch 9/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 193.9221 - val_loss: 196.8996\n",
            "Epoch 10/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 191.8119 - val_loss: 191.7241\n",
            "Epoch 11/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 190.1236 - val_loss: 191.4566\n",
            "Epoch 12/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 188.1255 - val_loss: 190.2515\n",
            "Epoch 13/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 188.2451 - val_loss: 188.6730\n",
            "Epoch 14/15\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 186.2407 - val_loss: 186.3596\n",
            "Epoch 15/15\n",
            "59/59 [==============================] - 2s 28ms/step - loss: 184.1037 - val_loss: 187.1351\n",
            "\n",
            "Training for digit:  9\n",
            "----------------------------------------\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 6s 100ms/step - loss: 285.2627 - val_loss: 0.0000e+00\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 195.1399 - val_loss: 182.7298\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 2s 26ms/step - loss: 177.1926 - val_loss: 175.2426\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 172.1300 - val_loss: 171.0134\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 170.8165 - val_loss: 169.7966\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 166.1267 - val_loss: 164.3915\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 162.8017 - val_loss: 164.8194\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 161.7886 - val_loss: 162.9143\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 160.4791 - val_loss: 162.1805\n",
            "Epoch 10/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 158.8760 - val_loss: 162.0695\n",
            "Epoch 11/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 157.2955 - val_loss: 158.8957\n",
            "Epoch 12/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 155.5673 - val_loss: 155.6534\n",
            "Epoch 13/15\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 153.8517 - val_loss: 153.9173\n",
            "Epoch 14/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 152.1977 - val_loss: 152.8199\n",
            "Epoch 15/15\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 150.6129 - val_loss: 154.0561\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCfxB6CCYGuu",
        "colab_type": "code",
        "outputId": "95bb751b-74db-4bcd-d954-523f0e564543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "d = 8\n",
        "test_seq = tf.cast(test_images[d][:1,...],np.float32)\n",
        "samp = models[d](test_seq).mean()[0,...,0]\n",
        "\n",
        "plt.imshow(samp)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fefd6ef9550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYtZOdqb5XIx",
        "colab_type": "text"
      },
      "source": [
        "# Counterfactual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd2iW4NS2t7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, d1, d2 = 12, 2, 5\n",
        "test_img = tf.cast(test_images[d1][:N,...],np.float32)\n",
        "transfer = decoders[d2](encoders[d1](test_img)).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoWHJs_8usMq",
        "colab_type": "code",
        "outputId": "240f6851-7f18-4de1-cddf-c79250f0fff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "fig, ax = plt.subplots(2,N, figsize=(N*2,5))\n",
        "for n in range(N):\n",
        "  ax[0,n].imshow(test_img[n,...,0])\n",
        "  ax[1,n].imshow(transfer[n,...,0])\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABrgAAAE5CAYAAADLHSBNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAAAAFLl/wFtqkrOlN8YlwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1728x360 with 24 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rD_Uw368KdZ",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEG5oF3R9YnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(imgs, models, D=10):\n",
        "  return tfm.argmax(tf.stack([models[d](imgs).log_prob(imgs) for d in range(D)]),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avy2f_meLSNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion(test_images, models, batch_size=100, D=10):\n",
        "  confusion_matrix = np.zeros((D,D))\n",
        "  for d in range(D):\n",
        "    sz = test_images[d].shape[0]\n",
        "    for b_id in range(0,sz,batch_size):\n",
        "      test_img = tf.cast(test_images[d][b_id:min(sz,b_id + batch_size),...], np.float32)\n",
        "      classes = classify(test_img, models, D).numpy()\n",
        "      for d_p in range(D):\n",
        "        confusion_matrix[d,d_p] += np.sum(classes == d_p)\n",
        "  return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDX6zAax8I3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = confusion(test_images,models,BATCH_SIZE,D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ogPlbY9zJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "1ba96083-ac91-4080-973f-2d7346589640"
      },
      "source": [
        "acc = np.sum(np.diag(conf))/np.sum(conf)\n",
        "print(acc)\n",
        "\n",
        "plt.imshow(conf)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fefd46ca390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBxIjcCAxAgcSI3AgMQIHEiNwILH/As3Aa0vAreSrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufRoHzVsnS5W",
        "colab_type": "text"
      },
      "source": [
        "# Discriminative Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2efcwlZMlOJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_discriminator(input_shape, latent_dim, base_filters, D=10):\n",
        "    discriminator = tfk.Sequential([\n",
        "      tfkl.InputLayer(input_shape=input_shape),\n",
        "      tfkl.Lambda(lambda x: x - 0.5),\n",
        "      tfkl.Conv2D(base_filters, 5, strides=2,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "      #tfkl.Conv2D(base_filters, 5, strides=2,\n",
        "      #          padding='same', activation=tf.nn.leaky_relu),\n",
        "      #tfkl.Conv2D(2 * base_filters, 5, strides=1,\n",
        "      #          padding='same', activation=tf.nn.leaky_relu),\n",
        "      tfkl.Conv2D(2 * base_filters, 5, strides=2,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "      tfkl.Conv2D(4 * latent_dim, 7, strides=2,\n",
        "                padding='valid', activation=tf.nn.leaky_relu),\n",
        "      tfkl.Flatten(),\n",
        "      tfkl.Dense(D, activation=\"softmax\"),\n",
        "      #tfpl.DistributionLambda(lambda p: tfd.Categorical(probs=p))\n",
        "      ])\n",
        "    return discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_mPt1BK_njZd",
        "colab": {}
      },
      "source": [
        "def minibatches_with_labels(data,labels,mb_size=1):\n",
        "  return tf.data.Dataset.from_tensor_slices((data,labels)).batch(mb_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4swTDm8N82V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = preprocess(train_images)\n",
        "test_images = preprocess(test_images)\n",
        "train_labels = train_labels.astype(\"int32\")\n",
        "test_labels = test_labels.astype(\"int32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_zSXnogqX7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = 10\n",
        "img_shape = train_images.shape[1:]\n",
        "encoded_size = 8\n",
        "base_depth = 24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY0upz7Wo51d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "train_dataset = minibatches_with_labels(train_images,train_labels,BATCH_SIZE)\n",
        "test_dataset = minibatches_with_labels(test_images,test_labels,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ffuJz2piY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "da98cf6e-abb6-4cd9-ceb4-7d27453f5997"
      },
      "source": [
        "discriminator = create_discriminator(img_shape, encoded_size, base_depth, D)\n",
        "\n",
        "discriminator.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "discriminator.fit(train_dataset, epochs=15, validation_data=test_dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "600/600 [==============================] - 8s 13ms/step - loss: 0.2732 - accuracy: 0.9172 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/15\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0906 - accuracy: 0.9723 - val_loss: 0.0787 - val_accuracy: 0.9720\n",
            "Epoch 3/15\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0643 - accuracy: 0.9805 - val_loss: 0.0723 - val_accuracy: 0.9754\n",
            "Epoch 4/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0495 - accuracy: 0.9852 - val_loss: 0.0678 - val_accuracy: 0.9775\n",
            "Epoch 5/15\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0394 - accuracy: 0.9881 - val_loss: 0.0683 - val_accuracy: 0.9790\n",
            "Epoch 6/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0321 - accuracy: 0.9902 - val_loss: 0.0695 - val_accuracy: 0.9785\n",
            "Epoch 7/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0268 - accuracy: 0.9916 - val_loss: 0.0738 - val_accuracy: 0.9789\n",
            "Epoch 8/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0228 - accuracy: 0.9922 - val_loss: 0.0709 - val_accuracy: 0.9815\n",
            "Epoch 9/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0192 - accuracy: 0.9936 - val_loss: 0.0751 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.0786 - val_accuracy: 0.9806\n",
            "Epoch 11/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.0997 - val_accuracy: 0.9790\n",
            "Epoch 12/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0915 - val_accuracy: 0.9796\n",
            "Epoch 13/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 0.0761 - val_accuracy: 0.9843\n",
            "Epoch 14/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0707 - val_accuracy: 0.9851\n",
            "Epoch 15/15\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.1114 - val_accuracy: 0.9788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fefd4637da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggO91rPoF5tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}